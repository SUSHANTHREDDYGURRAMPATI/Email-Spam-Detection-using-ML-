{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUSHANTHREDDYGURRAMPATI/Email-Spam-Detection-using-ML-/blob/main/Email_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWFKEVh4RvVP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpko4FPLRz-u"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/input\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuS5CAvjSD3q"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\n",
        "print ('show what kind of data we are dealing with')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG5Fg-aRSHuA"
      },
      "outputs": [],
      "source": [
        "print (df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZNFUjtITLWd"
      },
      "outputs": [],
      "source": [
        "data_train, data_test, labels_train, labels_test = train_test_split(\n",
        "    df.v2,\n",
        "    df.v1,\n",
        "    test_size=0.2,\n",
        "    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu6eHuUuTPqZ"
      },
      "outputs": [],
      "source": [
        "print('')\n",
        "print ('Now print each SMS text after train/test split')\n",
        "print (data_train[:10])\n",
        "print('')\n",
        "print ('Now print labels of SMS texts')\n",
        "print (labels_train[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl1sD06jTTTT"
      },
      "outputs": [],
      "source": [
        "def GetVocabulary(data):\n",
        "    vocab_set = set([])\n",
        "    for document in data:\n",
        "        words = document.split()\n",
        "        for word in words:\n",
        "            vocab_set.add(word)\n",
        "    return list(vocab_set)\n",
        "\n",
        "vocab_list = GetVocabulary(data_train)\n",
        "print ('Number of all the unique words : ' + str(len(vocab_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT59zOXJThrt"
      },
      "outputs": [],
      "source": [
        "def Document2Vector(vocab_list, data):\n",
        "    word_vector = np.zeros(len(vocab_list))\n",
        "    words = data.split()\n",
        "    for word in words:\n",
        "        if word in vocab_list:\n",
        "            word_vector[vocab_list.index(word)] += 1\n",
        "    return word_vector\n",
        "\n",
        "print (data_train[1:2,])\n",
        "ans = Document2Vector(vocab_list,\"the the the\")\n",
        "print (data_train.values[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q9HwYgTQToKk",
        "outputId": "6b2e957b-6f98-4edd-c3d1-cd256965fa1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4457\n"
          ]
        }
      ],
      "source": [
        "train_matrix = []\n",
        "for document in data_train.values:\n",
        "    word_vector = Document2Vector(vocab_list, document)\n",
        "    train_matrix.append(word_vector)\n",
        "\n",
        "print (len(train_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkbOe6XjTtJZ"
      },
      "outputs": [],
      "source": [
        "def NaiveBayes_train(train_matrix,labels_train):\n",
        "    num_docs = len(train_matrix)\n",
        "    num_words = len(train_matrix[0])\n",
        "\n",
        "    spam_vector_count = np.ones(num_words);\n",
        "    ham_vector_count = np.ones(num_words)  #initialize the count as 1 for each word\n",
        "    spam_total_count = num_words;\n",
        "    ham_total_count = num_words                  #this is Laplacian smooth\n",
        "\n",
        "    spam_count = 0\n",
        "    ham_count = 0\n",
        "    for i in range(num_docs):\n",
        "        if i % 500 == 0:\n",
        "            print ('Train on the doc id:' + str(i))\n",
        "\n",
        "        if labels_train[i] == 'spam':\n",
        "            ham_vector_count += train_matrix[i]\n",
        "            ham_total_count += sum(train_matrix[i])\n",
        "            ham_count += 1\n",
        "        else:\n",
        "            spam_vector_count += train_matrix[i]\n",
        "            spam_total_count += sum(train_matrix[i])\n",
        "            spam_count += 1\n",
        "\n",
        "    print (ham_count)\n",
        "    print (spam_count)\n",
        "\n",
        "    p_spam_vector = np.log(ham_vector_count/ham_total_count)#return probability vector\n",
        "    p_ham_vector = np.log(spam_vector_count/spam_total_count)#return a priori probabiligy\n",
        "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n",
        "\n",
        "\n",
        "p_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzLzxcX3T0TR"
      },
      "outputs": [],
      "source": [
        "data_test.values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LngFASS9T345"
      },
      "outputs": [],
      "source": [
        "def Predict(test_word_vector,p_spam_vector, p_spam, p_ham_vector, p_ham):\n",
        "\n",
        "    spam = sum(test_word_vector * p_spam_vector) + p_spam\n",
        "    ham = sum(test_word_vector * p_ham_vector) + p_ham\n",
        "    if spam > ham:\n",
        "        return 'spam'\n",
        "    else:\n",
        "        return 'ham'\n",
        "\n",
        "predictions = []\n",
        "i = 0\n",
        "for document in data_test.values:\n",
        "    if i % 200 == 0:\n",
        "        print ('Test on the doc id:' + str(i))\n",
        "    i += 1\n",
        "    test_word_vector = Document2Vector(vocab_list, document)\n",
        "    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n",
        "    predictions.append(ans)\n",
        "\n",
        "print (len(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpo6aPdwT8Dw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "print (accuracy_score(labels_test, predictions))\n",
        "print (classification_report(labels_test, predictions))\n",
        "print (confusion_matrix(labels_test, predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}